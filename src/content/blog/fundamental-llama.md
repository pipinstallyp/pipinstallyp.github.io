---
author: pipinstallyp
pubDatetime: 2022-09-23T15:22:00Z
title: Fundamental Developments in Llama
postSlug: fundamental-developments-in-llama
featured: true
draft: false
tags:
  - developments,  
  - llama
ogImage: ""
description: A look at some significant developments, tools and techniques in Llama.
---

A roundup of considerable developments, tools, and techniques in Llama landscape. 

## Table of Contents

## MEGABYTE
Predicting Million-byte Sequences with Multiscale Transformers [Read paper here](https://arxiv.org/abs/2305.07185)

## QLoRA
1. 4-bit finetuning of LLMs is here! With it comes Guanaco, a chatbot on a single GPU, achieving 99% ChatGPT performance on the Vicuna benchmark [Find it here](https://github.com/artidoro/qlora)
2. This is a bits and bytes release that can finetune 65B llama on single GPU.

## Sophia
A new optimizer that is 2x faster than Adam on LLMs [Read further here](https://arxiv.org/abs/2305.14342)

## AMD Support
Technique that makes llama cpp run on AMD [Check it out](https://github.com/ggerganov/llama.cpp/pull/1459)

## Context Free Grammar Constraints
Make your llama follow certain grammar [Learn more here](https://github.com/grantslatton/llama.cpp/commit/007e26a99d485007f724957fa8545331ab8d50c3)

## Reading to Understand Lora
[Click here](https://lightning.ai/pages/community/tutorial/lora-llm/)

## Repo
Repo that runs llama at 2x speed [Find here](https://github.com/turboderp/exllama) 

## LIMA
Training using 1000 instructions which donâ€™t involve RLHF to achieve near GPT-4 level performance in looks. Note that not all GPT-4 features are available as GPT-4 is too huge. [Read paper here](https://arxiv.org/pdf/2305.11206.pdf)